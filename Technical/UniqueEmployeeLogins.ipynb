{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, TimestampType\n",
    "from pyspark.sql.functions import col, to_timestamp, lit\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"WorkerLogins\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"worker_id\", LongType(), True),\n",
    "    StructField(\"login_timestamp\", TimestampType(), True),\n",
    "    StructField(\"ip_address\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"device_type\", StringType(), True)\n",
    "])\n",
    "# Sample Data\n",
    "data = [\n",
    "    (1, 101, datetime(2024, 2, 15, 8, 30, 0), \"192.168.1.1\", \"USA\", \"California\", \"Los Angeles\", \"Laptop\"),\n",
    "    (2, 102, datetime(2024, 2, 15, 9, 15, 0), \"192.168.1.2\", \"Canada\", \"Ontario\", \"Toronto\", \"Mobile\"),\n",
    "    (3, 103, datetime(2024, 2, 16, 12, 45, 0), \"192.168.1.3\", \"UK\", \"England\", \"London\", \"Tablet\"),\n",
    "    (4, 104, datetime(2024, 2, 17, 14, 10, 0), \"192.168.1.4\", \"India\", \"Maharashtra\", \"Mumbai\", \"Laptop\"),\n",
    "    (5, 105, datetime(2024, 2, 18, 16, 55, 0), \"192.168.1.5\", \"Germany\", \"Bavaria\", \"Munich\", \"Mobile\")\n",
    "]\n",
    "\n",
    "worker_logins = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Start writing code\n",
    "df_worker_logins = worker_logins.select(\"worker_id\").filter(\n",
    "    (col(\"login_timestamp\") >= to_timestamp(lit(\"2024-02-10 00:00:00\"))) & \n",
    "    (col(\"login_timestamp\") <= to_timestamp(lit(\"2024-02-15 23:59:59\")))\n",
    ")\n",
    "\n",
    "# To validate your solution, convert your final pySpark df to a pandas df\n",
    "#worker_logins.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------------+-----------+-------+-----------+-----------+-----------+\n",
      "| id|worker_id|    login_timestamp| ip_address|country|     region|       city|device_type|\n",
      "+---+---------+-------------------+-----------+-------+-----------+-----------+-----------+\n",
      "|  1|      101|2024-02-15 08:30:00|192.168.1.1|    USA| California|Los Angeles|     Laptop|\n",
      "|  2|      102|2024-02-15 09:15:00|192.168.1.2| Canada|    Ontario|    Toronto|     Mobile|\n",
      "|  3|      103|2024-02-16 12:45:00|192.168.1.3|     UK|    England|     London|     Tablet|\n",
      "|  4|      104|2024-02-17 14:10:00|192.168.1.4|  India|Maharashtra|     Mumbai|     Laptop|\n",
      "|  5|      105|2024-02-18 16:55:00|192.168.1.5|Germany|    Bavaria|     Munich|     Mobile|\n",
      "+---+---------+-------------------+-----------+-------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worker_logins.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|worker_id|\n",
      "+---------+\n",
      "|      101|\n",
      "|      102|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_worker_logins.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession.stop(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
