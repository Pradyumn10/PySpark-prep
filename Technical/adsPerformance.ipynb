{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ads Performance\n",
    "\n",
    "Write an pyspark code to find the ctr of each Ad.Round ctr to 2 decimal points. Order the result table by ctr in descending order and by ad_id in ascending order in case of a tie.\n",
    "Ctr=Clicked/(Clicked+Viewed)\n",
    "\n",
    "#### Difficult Level : EASY\n",
    "##### DataFrame:\n",
    "schema=StructType([\n",
    "StructField('AD_ID',IntegerType(),True)\n",
    ",StructField('USER_ID',IntegerType(),True)\n",
    ",StructField('ACTION',StringType(),True)\n",
    "])\n",
    "# Define the data for the Ads table\n",
    "data = [\n",
    "(1, 1, 'Clicked'),\n",
    "(2, 2, 'Clicked'),\n",
    "(3, 3, 'Viewed'),\n",
    "(5, 5, 'Ignored'),\n",
    "(1, 7, 'Ignored'),\n",
    "(2, 7, 'Viewed'),\n",
    "(3, 5, 'Clicked'),\n",
    "(1, 4, 'Viewed'),\n",
    "(2, 11, 'Viewed'),\n",
    "(1, 2, 'Clicked')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StructField,StringType, StructType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.master(\"local[*]\").appName(\"AdsPerformance\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|AD_ID|USER_ID| ACTION|\n",
      "+-----+-------+-------+\n",
      "|    1|      1|Clicked|\n",
      "|    2|      2|Clicked|\n",
      "|    3|      3| Viewed|\n",
      "|    5|      5|Ignored|\n",
      "|    1|      7|Ignored|\n",
      "|    2|      7| Viewed|\n",
      "|    3|      5|Clicked|\n",
      "|    1|      4| Viewed|\n",
      "|    2|     11| Viewed|\n",
      "|    1|      2|Clicked|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema=StructType([\n",
    "StructField('AD_ID',IntegerType(),True)\n",
    ",StructField('USER_ID',IntegerType(),True)\n",
    ",StructField('ACTION',StringType(),True)\n",
    "])\n",
    "# Define the data for the Ads table\n",
    "data = [\n",
    "(1, 1, 'Clicked'),\n",
    "(2, 2, 'Clicked'),\n",
    "(3, 3, 'Viewed'),\n",
    "(5, 5, 'Ignored'),\n",
    "(1, 7, 'Ignored'),\n",
    "(2, 7, 'Viewed'),\n",
    "(3, 5, 'Clicked'),\n",
    "(1, 4, 'Viewed'),\n",
    "(2, 11, 'Viewed'),\n",
    "(1, 2, 'Clicked')\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+\n",
      "|AD_ID|Clicked|Viewed|\n",
      "+-----+-------+------+\n",
      "|    1|      2|     1|\n",
      "|    2|      1|     2|\n",
      "|    3|      1|     1|\n",
      "|    5|      0|     0|\n",
      "+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_filter=df.groupBy('AD_ID').agg(F.sum(F.when(F.col('ACTION')=='Clicked',1).otherwise(0)).alias('Clicked'),F.sum(F.when(F.col('ACTION')=='Viewed',1).otherwise(0)).alias('Viewed'))\n",
    "df_filter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|AD_ID| ctr|\n",
      "+-----+----+\n",
      "|    1|0.67|\n",
      "|    2|0.33|\n",
      "|    3| 0.5|\n",
      "|    5|NULL|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFinal=df_filter.withColumn('ctr',F.round(F.col('Clicked')/(F.col('Clicked')+F.col('Viewed')),2)).select('AD_ID','ctr')\n",
    "dfFinal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|AD_ID| ctr|\n",
      "+-----+----+\n",
      "|    1|0.67|\n",
      "|    2|0.33|\n",
      "|    3| 0.5|\n",
      "|    5|NULL|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.orderBy(F.col(\"ctr\").desc(), F.col(\"ad_id\").asc())\n",
    "result_df = dfFinal.withColumn(\"rank\", F.rank().over(window_spec)).select('AD_ID','ctr')\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
